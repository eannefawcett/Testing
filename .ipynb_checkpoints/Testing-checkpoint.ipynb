{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from math import floor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_and_urls(website_list):\n",
    "    '''To acquire a list of titles and website extenstions for gutenberg.org from the Bookshelves part of the website. \n",
    "    Requires a list of website urls to be passed in. \n",
    "    Returns titles and link extensions'''\n",
    "    titles = []\n",
    "    links = []\n",
    "    for counter, website in enumerate(website_list):\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.implicitly_wait(5)\n",
    "        driver.get(website)\n",
    "        for x, y  in enumerate(driver.find_elements_by_xpath('./html/body/div[1]/div[2]/div[2]/div/ul/li')):\n",
    "            try:\n",
    "                if [i.get_attribute('alt') for i in y.find_elements_by_xpath('./a[@class=\"image\"]/img')][0] == 'BookIcon.png':\n",
    "                    titles.append(y.find_elements_by_xpath('./a')[0].text)\n",
    "                    links.append(y.find_elements_by_xpath('./a')[0].get_attribute('href'))\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "        driver.quit()\n",
    "        print('Website', counter+1, 'information acquired!')\n",
    "    print('Is length of titles scrapped and links scrapped equal?', len(titles)==len(links))\n",
    "    return titles, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\"https://www.gutenberg.org/wiki/Children%27s_Literature_(Bookshelf)\", \n",
    "            \"https://www.gutenberg.org/wiki/Children%27s_Fiction_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Adventure_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Fantasy_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Humor_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Mystery_Fiction_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Movie_Books_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Science_Fiction_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Children%27s_History_(Bookshelf)\",\n",
    "            \"https://www.gutenberg.org/wiki/Plays_(Bookshelf)\"]\n",
    "titles, links = get_titles_and_urls(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_search = []\n",
    "for x in links:\n",
    "    links_to_search.append(x.split('/')[4])\n",
    "len(links_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "urls_used = []\n",
    "urls_not_used = []\n",
    "for i in range(0, len(links_to_search)):\n",
    "    to_find = links_to_search[i]\n",
    "    print(i)\n",
    "    try:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.implicitly_wait(5)\n",
    "        driver.get(\"https://www.gutenberg.org/ebooks/\"+to_find+\".txt.utf-8\")\n",
    "        text = driver.find_element_by_xpath('//pre')\n",
    "        texts.append(text.text)\n",
    "        driver.quit()\n",
    "        urls_used.append(i)\n",
    "        print('Text acquired!')\n",
    "    except NoSuchElementException as inst:\n",
    "        print(inst)          # __str__ allows args to be printed directly,\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.implicitly_wait(5)\n",
    "        driver.get(\"https://www.gutenberg.org/files/\"+to_find+\"/\"+to_find+\"-0.txt\")\n",
    "        text = driver.find_element_by_xpath('//pre')\n",
    "        texts.append(text.text)\n",
    "        driver.quit()\n",
    "        urls_used.append(i)\n",
    "        print('Text acquired!')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('full_texts.pickle', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([titles, texts], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('full_texts.pickle', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_titles, full_text_texts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cases = [57, 158, 189, 189]\n",
    "for x in special_cases:\n",
    "    full_text_titles.pop(x)\n",
    "    full_text_texts.pop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(5)\n",
    "driver.get(\"https://hub.lexile.com/find-a-book/search\")\n",
    "analytics = driver.find_element_by_xpath('//label[@for=\"checkbox_1\"]')\n",
    "result = analytics.is_selected()\n",
    "if result:\n",
    "    print('Checkbox already selected')\n",
    "else:\n",
    "    analytics.click()\n",
    "    print('Analytics Checkbox deselected.')\n",
    "save_analytics_button = driver.find_element_by_xpath('//button[text()=\"Save Preferences\"]')\n",
    "save_analytics_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "authors = []\n",
    "lexiles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(full_text_titles)):\n",
    "    print(x)\n",
    "    value = str(full_text_titles[x]).replace(\"/\", ' ')\n",
    "    search_bar = driver.find_element_by_xpath('//input[@name=\"quickSearch\"]')\n",
    "    search_bar.send_keys(value)\n",
    "    search_button = driver.find_element_by_xpath('//button[text()=\"Search\"]')\n",
    "    search_button.click()\n",
    "    print('Finding books')\n",
    "    titles = driver.find_elements_by_xpath('//*[@data-testid=\"book-title\"]')\n",
    "    book_titles = []\n",
    "    for result in titles:\n",
    "        book_titles.append(result.text)\n",
    "    print('Finding authors')\n",
    "    authors_of_books = driver.find_elements_by_xpath('//*[@data-testid=\"book-authors\"]')\n",
    "    book_authors = []\n",
    "    for result in authors_of_books:\n",
    "        book_authors.append(result.text)\n",
    "    print('Finding levels')\n",
    "    levels = driver.find_elements_by_class_name('sc-iFMziU.ixIPnd')\n",
    "    lexile_levels = []\n",
    "    for result in levels:\n",
    "        lexile_levels.append(result.text)\n",
    "    books.append(book_titles)\n",
    "    authors.append(book_authors)\n",
    "    lexiles.append(lexile_levels)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    element = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[text()=\"New Search\"]')))\n",
    "    element.click()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('lexile_levels.pickle', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([books, authors, lexiles], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse1 = []\n",
    "parse2 = []\n",
    "parse3 = []\n",
    "parse4 = []\n",
    "titles_from_text = []\n",
    "for x in range(0, len(full_text_texts)):\n",
    "#     print('1. removing license agreement at end')\n",
    "    parse1.append(re.sub('\\n', ' ', full_text_texts[x].splitlines()))\n",
    "#     print('2. removing extras at the end of the text')\n",
    "    if len(parse1[x].split('THE END'))==2:\n",
    "        parse2.append(parse1[x].split('THE END')[0])\n",
    "    elif len(parse1[x].split(\"End of the Project Gutenberg EBook\"))==2:\n",
    "        parse2.append(parse1[x].split(\"End of the Project Gutenberg EBook\")[0])\n",
    "    elif len(parse1[x].split('The End'))==2:\n",
    "        parse2.append(parse1[x].split('The End')[0])\n",
    "    elif len(parse1[x].split('End of Project Gutenberg\\'s'))==2:\n",
    "        parse2.append(parse1[x].split('End of Project Gutenberg\\'s')[0])\n",
    "    elif len(parse1[x].split('END OF THE PROJECT GUTENBERG EBOOK'))==2:\n",
    "        parse2.append(parse1[x].split('END OF THE PROJECT GUTENBERG EBOOK')[0]) \n",
    "    elif len(parse1[x].split('END OF THIS PROJECT GUTENBERG EBOOK'))==2:\n",
    "        parse2.append(parse1[x].split('END OF THIS PROJECT GUTENBERG EBOOK')[0])\n",
    "    elif len(parse1[x].split('The Project Gutenberg Etext'))==4:\n",
    "        parse2.append(parse1[x].split('The Project Gutenberg Etext')[2])\n",
    "    elif len(parse1[x].split('*Project Gutenberg Etext'))==4:\n",
    "        parse2.append(parse1[x].split('*Project Gutenberg Etext')[2])\n",
    "    else:\n",
    "        print('problem at: ', x)\n",
    "#     print('3. removing anything at ebook related at beginning of text')\n",
    "    if len(parse2[x].split('START OF THIS PROJECT GUTENBERG EBOOK'))==2:\n",
    "        parse3.append(parse2[x].split('START OF THIS PROJECT GUTENBERG EBOOK')[1])\n",
    "    elif len(parse2[x].split(\"START OF THE PROJECT GUTENBERG EBOOK\"))==2:\n",
    "        parse3.append(parse2[x].split(\"START OF THE PROJECT GUTENBERG EBOOK\")[1])\n",
    "    elif len(parse2[x].split(\"THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A TIME WHEN PROOFING METHODS AND TOOLS WERE NOT WELL DEVELOPED.\"))==2:\n",
    "        parse3.append(parse2[x].split(\"THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A TIME WHEN PROOFING METHODS AND TOOLS WERE NOT WELL DEVELOPED.\")[1])\n",
    "    elif len(parse2[x].split(\"THE SMALL PRINT! FOR PUBLIC DOMAIN ETEXTS\"))==2:\n",
    "        parse3.append(parse2[x].split(\"THE SMALL PRINT! FOR PUBLIC DOMAIN ETEXTS\")[1].split('*END*')[1])\n",
    "    else:\n",
    "        print('problem at: ', x)\n",
    "    if len(parse3[x].split('***'))==2:\n",
    "        parse4.append(parse3[x].split('***')[1])\n",
    "    elif len(parse3[x].split('***'))==48:\n",
    "        parse4.append(parse3[x].split('***')[47].split('*')[1])\n",
    "    elif len(parse3[x].split('***'))==3:\n",
    "        if len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team'))==2:\n",
    "            parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[1])\n",
    "        elif len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team'))==1:\n",
    "            if len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)'))==2:\n",
    "                parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[1])\n",
    "            elif len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)'))==1:\n",
    "                if len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.org'))==2:\n",
    "                    parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.org')[1])\n",
    "                elif len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.ac.uk'))==2:\n",
    "                    parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.ac.uk')[1])\n",
    "                elif len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.edu/women/.'))==2:\n",
    "                    parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('.edu/women/.')[0])\n",
    "                elif len(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('     '))==839:\n",
    "                    parse4.append(parse3[x].split('***')[1].split('Online Distributed Proofreading Team')[0].split('.zip)')[0].split('This eBook was')[1])\n",
    "                else:\n",
    "                    parse4.append(parse3[x])\n",
    "            else:\n",
    "                print('review: ', x)\n",
    "        else:\n",
    "            print('look at: ', x)\n",
    "    elif len(parse3[x].split('***'))==7:\n",
    "        parse4.append(parse3[x])\n",
    "    else:\n",
    "        print('problem at: ', x)\n",
    "    print('Book number ', x, ' done!')\n",
    "    titles_from_text.append(parse3[x].split('*** ')[0].strip().title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('lexile_levels.pickle', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, authors, lexiles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_books = []\n",
    "for i, j in enumerate(books):\n",
    "    try:\n",
    "        search_results_books.append(j[0].title())\n",
    "    except:\n",
    "        search_results_books.append('')\n",
    "search_results_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_lexiles = []\n",
    "for i, j in enumerate(lexiles):\n",
    "    try:\n",
    "        search_results_lexiles.append(j[0])\n",
    "    except:\n",
    "        search_results_lexiles.append('')\n",
    "search_results_lexiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_lexiles = []\n",
    "for x in search_results_lexiles:\n",
    "    length_lexiles.append(x!='')\n",
    "print('Number of scraped texts with associated lexile levels: ', sum(length_lexiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.DataFrame()\n",
    "labeled_lexiles = []\n",
    "labeled_titles = []\n",
    "labeled_text = []\n",
    "for x in range(0, len(search_results_lexiles)):\n",
    "    if str(search_results_books[x]) == str(titles_from_text[x]):\n",
    "        labeled_lexiles.append(search_results_lexiles[x])\n",
    "        labeled_titles.append(titles_from_text[x])\n",
    "        labeled_text.append(parse4[x])\n",
    "#         labels = list(zip(search_results_lexiles[x], titles_from_text[x]))\n",
    "    else:\n",
    "        print(x, 'No title or lexile match to scrapped material')\n",
    "labeled_data['Lexiles'] = labeled_lexiles\n",
    "labeled_data['Titles'] = labeled_titles\n",
    "labeled_data['Texts'] = labeled_text\n",
    "print('Number of scraped texts that can be labeled with lexile levels: ', len(labeled_data))\n",
    "labeled_data.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
